# TODOs for Livewell Case Study

The purpose of the TODOs is to bridge the gap between the Livewell case study problem and the code implementations + markdown content (words and diagrams).

## Production Architecture for Scale (Thousands of Concurrent Requests)

### Current System Analysis
The current system provides a solid foundation:
- âœ… Hybrid deterministic-agentic architecture  
- âœ… FastAPI with Redis rate limiting and concurrency controls
- âœ… Multiple interfaces (CLI, REST API, MCP server)
- âœ… Comprehensive observability with Weave tracing
- âœ… Safety-first design with audit trails

**Gap Analysis for Production Scale:**
- âš ï¸ Single-instance deployment (no horizontal scaling)
- âš ï¸ Basic Redis setup (needs clustering for HA)  
- âš ï¸ No API Gateway or advanced load balancing
- âš ï¸ Limited caching strategy (only rate limiting)
- âš ï¸ No auto-scaling or resource optimization
- âš ï¸ Basic health checks (needs comprehensive monitoring)
- âš ï¸ No deployment automation or CI/CD
- âš ï¸ Database layer not optimized for high concurrency

### Production-Ready Architecture Overview

```mermaid
flowchart TB
    %% External Traffic
    Users["ğŸ‘¥ Users<br/>(Thousands concurrent)"]
    
    %% Edge Layer
    CDN["ğŸŒ CDN (CloudFlare)<br/>Static Assets<br/>Response Caching"]
    WAF["ğŸ›¡ï¸ Web Application Firewall<br/>DDoS Protection<br/>Security Rules"]
    
    %% API Gateway Layer
    Gateway["ğŸšª API Gateway<br/>(Kong/AWS ALB)<br/>Rate Limiting<br/>Authentication<br/>Request Routing<br/>Circuit Breaking"]
    
    %% Load Balancing
    ALB["âš–ï¸ Application Load Balancer<br/>Health Checks<br/>SSL Termination<br/>Connection Pooling"]
    
    %% Application Layer (Auto-scaled)
    subgraph "ğŸš€ Application Tier (Auto-scaling)"
        API1["ğŸ–¥ï¸ UTI API Instance 1<br/>FastAPI + Uvicorn<br/>Async Workers"]
        API2["ğŸ–¥ï¸ UTI API Instance 2<br/>FastAPI + Uvicorn<br/>Async Workers"]  
        API3["ğŸ–¥ï¸ UTI API Instance N<br/>FastAPI + Uvicorn<br/>Async Workers"]
    end
    
    %% Caching Layer
    subgraph "âš¡ Caching Layer"
        RedisCluster["ğŸ”´ Redis Cluster<br/>Session Store<br/>Rate Limiting<br/>Application Cache<br/>HA Setup"]
        Memcached["ğŸ’¾ Memcached<br/>Response Cache<br/>LLM Results Cache<br/>Hot Data"]
    end
    
    %% Message Queue & Async Processing
    subgraph "ğŸ“¨ Async Processing"
        Queue["ğŸ”„ Message Queue<br/>(Redis Streams/RabbitMQ)<br/>Background Jobs<br/>LLM Processing"]
        Workers["ğŸ‘· Background Workers<br/>Agent Processing<br/>Report Generation<br/>Evidence Synthesis"]
    end
    
    %% Data Layer
    subgraph "ğŸ—„ï¸ Data Layer"
        PrimaryDB["ğŸ“Š Primary Database<br/>(PostgreSQL)<br/>Patient Data<br/>Assessments<br/>Audit Logs"]
        ReadReplica1["ğŸ“– Read Replica 1<br/>Analytics Queries<br/>Report Generation"]
        ReadReplica2["ğŸ“– Read Replica 2<br/>Load Distribution"]
    end
    
    %% External Services
    subgraph "ğŸŒ External Services"
        LLMApis["ğŸ§  LLM APIs<br/>OpenAI GPT-4/GPT-5<br/>Connection Pooling<br/>Retry Logic<br/>Circuit Breakers"]
        Monitoring["ğŸ“Š Monitoring<br/>Prometheus<br/>Grafana<br/>AlertManager"]
        Logging["ğŸ“ Centralized Logging<br/>ELK Stack<br/>Structured Logs"]
        Tracing["ğŸ” Distributed Tracing<br/>Jaeger/Weave<br/>Performance Monitoring"]
    end
    
    %% Flow Connections
    Users --> CDN
    CDN --> WAF
    WAF --> Gateway
    Gateway --> ALB
    ALB --> API1
    ALB --> API2  
    ALB --> API3
    
    API1 --> RedisCluster
    API2 --> RedisCluster
    API3 --> RedisCluster
    
    API1 --> Memcached
    API2 --> Memcached
    API3 --> Memcached
    
    API1 --> Queue
    API2 --> Queue
    API3 --> Queue
    
    Queue --> Workers
    Workers --> LLMApis
    
    API1 --> PrimaryDB
    API2 --> ReadReplica1
    API3 --> ReadReplica2
    
    API1 --> Monitoring
    API2 --> Logging
    API3 --> Tracing
```

### Deployment Architecture (Kubernetes)

```mermaid
flowchart TB
    subgraph "â˜ï¸ Cloud Provider (AWS/GCP/Azure)"
        subgraph "ğŸ—ï¸ Kubernetes Cluster"
            subgraph "ğŸšª Ingress Layer"
                Ingress["ğŸ“¡ NGINX Ingress<br/>SSL Termination<br/>Load Balancing<br/>Path Routing"]
            end
            
            subgraph "ğŸ”„ Application Namespace"
                subgraph "ğŸ“± UTI API Deployment"
                    APIPods["ğŸƒ API Pods (3-20)<br/>Auto-scaling<br/>Resource Limits<br/>Health Checks"]
                end
                
                subgraph "ğŸ‘· Worker Deployment"
                    WorkerPods["ğŸ”§ Worker Pods (2-10)<br/>Agent Processing<br/>Background Tasks<br/>Queue Processing"]
                end
                
                subgraph "ğŸ“Š Services"
                    APIService["ğŸŒ API Service<br/>ClusterIP<br/>Load Balancing"]
                    WorkerService["ğŸ”— Worker Service<br/>Internal Communication"]
                end
            end
            
            subgraph "âš¡ Infrastructure Namespace"
                subgraph "ğŸ”´ Redis Deployment"
                    RedisPods["ğŸ—„ï¸ Redis Pods (3)<br/>Master-Slave<br/>Sentinel HA<br/>Persistent Storage"]
                end
                
                subgraph "ğŸ“Š Monitoring Stack"
                    PrometheusPods["ğŸ“ˆ Prometheus<br/>Metrics Collection"]
                    GrafanaPods["ğŸ“Š Grafana<br/>Dashboards"]
                    AlertManagerPods["ğŸš¨ AlertManager<br/>Notifications"]
                end
            end
        end
        
        subgraph "ğŸ—„ï¸ Managed Services"
            RDS["ğŸ˜ Managed PostgreSQL<br/>Multi-AZ<br/>Read Replicas<br/>Automated Backups"]
            S3["ğŸ’¾ Object Storage<br/>Static Files<br/>Backups<br/>Logs Archive"]
            CloudWatch["ğŸ“Š Cloud Monitoring<br/>Metrics<br/>Logs<br/>Alarms"]
        end
    end
    
    Ingress --> APIService
    APIService --> APIPods
    APIPods --> RedisPods
    APIPods --> RDS
    WorkerPods --> RDS
    WorkerPods --> RedisPods
```

### Scaling Strategy

```mermaid
flowchart LR
    subgraph "ğŸ“Š Auto-scaling Triggers"
        CPUMetrics["ğŸƒ CPU Usage > 70%"]
        MemoryMetrics["ğŸ’¾ Memory Usage > 80%"]
        RequestMetrics["ğŸ“ˆ Request Queue > 100"]
        ResponseTime["â±ï¸ Response Time > 2s"]
        RedisMetrics["ğŸ”´ Redis Connections > 80%"]
    end
    
    subgraph "ğŸ¯ Scaling Actions"
        HorizontalPodAuto["ğŸ“ˆ Horizontal Pod Autoscaler<br/>Scale API pods 3-20<br/>Scale worker pods 2-10"]
        VerticalPodAuto["ğŸ“Š Vertical Pod Autoscaler<br/>Adjust CPU/Memory limits<br/>Right-size containers"]
        ClusterAuto["ğŸ—ï¸ Cluster Autoscaler<br/>Add/remove nodes<br/>Cost optimization"]
    end
    
    subgraph "âš¡ Performance Optimization"
        ConnectionPool["ğŸŒŠ Connection Pooling<br/>Database connections<br/>Redis connections<br/>HTTP client pools"]
        RequestBatching["ğŸ“¦ Request Batching<br/>Batch LLM calls<br/>Database queries<br/>Cache operations"]
        AsyncProcessing["ğŸ”„ Async Processing<br/>Non-blocking I/O<br/>Background tasks<br/>Queue processing"]
    end
    
    CPUMetrics --> HorizontalPodAuto
    MemoryMetrics --> VerticalPodAuto
    RequestMetrics --> ClusterAuto
    ResponseTime --> ConnectionPool
    RedisMetrics --> RequestBatching
    
    HorizontalPodAuto --> AsyncProcessing
    VerticalPodAuto --> AsyncProcessing
```

### Caching Architecture

```mermaid
flowchart TD
    Request["ğŸ“¥ Incoming Request"]
    
    subgraph "ğŸŒ Edge Caching (CDN)"
        EdgeCache["âš¡ CDN Cache<br/>Static responses<br/>Common assessments<br/>TTL: 5-15 minutes"]
    end
    
    subgraph "ğŸšª API Gateway Cache"
        GatewayCache["ğŸ”„ Gateway Cache<br/>Rate limit responses<br/>Authentication tokens<br/>TTL: 1-5 minutes"]
    end
    
    subgraph "ğŸ“± Application Cache"
        L1Cache["ğŸ’¾ In-Memory Cache<br/>Hot patient data<br/>Algorithm results<br/>TTL: 30 seconds"]
        
        L2Cache["ğŸ”´ Redis Cache<br/>Patient sessions<br/>Assessment results<br/>LLM responses<br/>TTL: 15-60 minutes"]
        
        L3Cache["ğŸ’¿ Database Cache<br/>Query result cache<br/>Connection pooling<br/>Prepared statements"]
    end
    
    Database["ğŸ—„ï¸ PostgreSQL<br/>Persistent Data"]
    
    Request --> EdgeCache
    EdgeCache -->|Cache Miss| GatewayCache
    GatewayCache -->|Cache Miss| L1Cache
    L1Cache -->|Cache Miss| L2Cache
    L2Cache -->|Cache Miss| L3Cache
    L3Cache -->|Cache Miss| Database
    
    %% Cache warming flows
    Database -.->|Write-through| L3Cache
    L3Cache -.->|Write-through| L2Cache
    L2Cache -.->|Async update| L1Cache
```

### Database Optimization

```mermaid
flowchart TB
    subgraph "ğŸ“Š Database Architecture"
        subgraph "âœï¸ Write Operations"
            PrimaryDB["ğŸ† Primary Database<br/>All writes<br/>Patient assessments<br/>Audit logs<br/>Real-time consistency"]
        end
        
        subgraph "ğŸ“– Read Operations"
            ReadReplica1["ğŸ“š Read Replica 1<br/>Analytics queries<br/>Report generation<br/>Dashboard metrics"]
            
            ReadReplica2["ğŸ“° Read Replica 2<br/>Patient lookups<br/>Historical data<br/>Background processing"]
            
            ReadReplica3["ğŸ“„ Read Replica 3<br/>Audit queries<br/>Compliance reporting<br/>Data exports"]
        end
        
        subgraph "âš¡ Performance Optimization"
            ConnectionPool["ğŸŒŠ Connection Pooling<br/>PgBouncer<br/>Max 100 connections<br/>Statement caching"]
            
            Partitioning["ğŸ—‚ï¸ Table Partitioning<br/>By date (assessments)<br/>By region (patients)<br/>By status (audit_logs)"]
            
            Indexing["ğŸ” Strategic Indexing<br/>Patient lookups<br/>Assessment queries<br/>Audit trail searches"]
        end
    end
    
    Applications["ğŸ“± Application Pods"] --> ConnectionPool
    ConnectionPool --> PrimaryDB
    ConnectionPool --> ReadReplica1
    ConnectionPool --> ReadReplica2
    ConnectionPool --> ReadReplica3
    
    PrimaryDB --> Partitioning
    ReadReplica1 --> Indexing
```

### Monitoring & Observability

```mermaid
flowchart TB
    subgraph "ğŸ“Š Metrics Collection"
        AppMetrics["ğŸ“± Application Metrics<br/>Request rate<br/>Response time<br/>Error rate<br/>Active users"]
        
        InfraMetrics["ğŸ—ï¸ Infrastructure Metrics<br/>CPU/Memory usage<br/>Network I/O<br/>Disk usage<br/>Pod health"]
        
        BusinessMetrics["ğŸ’¼ Business Metrics<br/>Assessment success rate<br/>Agent decision quality<br/>Patient satisfaction<br/>Safety incidents"]
    end
    
    subgraph "ğŸ” Tracing & Logging"
        DistributedTracing["ğŸ•¸ï¸ Distributed Tracing<br/>Request flows<br/>Agent interactions<br/>LLM call tracking<br/>Performance bottlenecks"]
        
        StructuredLogs["ğŸ“ Structured Logging<br/>Application logs<br/>Audit trails<br/>Error tracking<br/>Security events"]
    end
    
    subgraph "ğŸ“ˆ Storage & Analysis"
        Prometheus["ğŸ“Š Prometheus<br/>Metrics storage<br/>Alerting rules<br/>Query engine"]
        
        ElasticSearch["ğŸ” ElasticSearch<br/>Log aggregation<br/>Full-text search<br/>Log analysis"]
        
        Grafana["ğŸ“Š Grafana<br/>Dashboards<br/>Visualizations<br/>Alerting"]
        
        Jaeger["ğŸ¯ Jaeger<br/>Trace storage<br/>Performance analysis<br/>Service maps"]
    end
    
    subgraph "ğŸš¨ Alerting"
        AlertManager["ğŸš¨ AlertManager<br/>Alert routing<br/>Notification channels<br/>Escalation policies"]
        
        PagerDuty["ğŸ“ PagerDuty<br/>Incident management<br/>On-call rotation<br/>Escalation"]
        
        Slack["ğŸ’¬ Slack<br/>Team notifications<br/>Status updates<br/>Collaboration"]
    end
    
    AppMetrics --> Prometheus
    InfraMetrics --> Prometheus
    BusinessMetrics --> Prometheus
    
    DistributedTracing --> Jaeger
    StructuredLogs --> ElasticSearch
    
    Prometheus --> Grafana
    ElasticSearch --> Grafana
    Jaeger --> Grafana
    
    Prometheus --> AlertManager
    AlertManager --> PagerDuty
    AlertManager --> Slack
```

### Security Architecture

```mermaid
flowchart TB
    subgraph "ğŸ›¡ï¸ External Security"
        WAF["ğŸ”¥ Web Application Firewall<br/>DDoS protection<br/>IP filtering<br/>Request validation<br/>Rate limiting"]
        
        CDNSecurity["ğŸŒ CDN Security<br/>SSL/TLS termination<br/>Certificate management<br/>Origin protection"]
    end
    
    subgraph "ğŸšª API Gateway Security"
        AuthN["ğŸ” Authentication<br/>JWT tokens<br/>OAuth2/OIDC<br/>API keys<br/>Session management"]
        
        AuthZ["ğŸ« Authorization<br/>RBAC policies<br/>Resource permissions<br/>Scope validation<br/>Context-aware access"]
        
        RateLimit["âš–ï¸ Rate Limiting<br/>Per-user limits<br/>Per-endpoint limits<br/>Burst protection<br/>Geographic limits"]
    end
    
    subgraph "ğŸ“± Application Security"
        InputValidation["âœ… Input Validation<br/>Pydantic schemas<br/>Data sanitization<br/>Type checking<br/>Boundary validation"]
        
        DataEncryption["ğŸ”’ Data Encryption<br/>At-rest encryption<br/>In-transit encryption<br/>Key management<br/>PII protection"]
        
        AuditLogging["ğŸ“‹ Audit Logging<br/>Access logs<br/>API calls<br/>Data changes<br/>Security events"]
    end
    
    subgraph "ğŸ—ï¸ Infrastructure Security"
        NetworkPolicies["ğŸŒ Network Policies<br/>Pod-to-pod security<br/>Ingress/Egress rules<br/>Service mesh<br/>Zero-trust networking"]
        
        SecretManagement["ğŸ—ï¸ Secret Management<br/>Kubernetes secrets<br/>External secret stores<br/>Rotation policies<br/>Least privilege"]
        
        PodSecurity["ğŸ›¡ï¸ Pod Security<br/>Security contexts<br/>Non-root users<br/>Read-only filesystems<br/>Resource limits"]
    end
    
    WAF --> CDNSecurity
    CDNSecurity --> AuthN
    AuthN --> AuthZ
    AuthZ --> RateLimit
    RateLimit --> InputValidation
    InputValidation --> DataEncryption
    DataEncryption --> AuditLogging
    AuditLogging --> NetworkPolicies
    NetworkPolicies --> SecretManagement
    SecretManagement --> PodSecurity
```

## Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2)
- [ ] **Containerization**: Create production Docker images with multi-stage builds
- [ ] **Kubernetes Setup**: Deploy to managed K8s cluster with basic scaling
- [ ] **Redis Clustering**: Implement Redis Cluster for high availability
- [ ] **Database Optimization**: Set up read replicas and connection pooling
- [ ] **Basic Monitoring**: Deploy Prometheus, Grafana, and basic alerts

### Phase 2: Scaling (Weeks 3-4)  
- [ ] **Auto-scaling**: Implement HPA, VPA, and cluster autoscaling
- [ ] **Advanced Caching**: Multi-tier caching strategy with TTL optimization
- [ ] **API Gateway**: Deploy Kong/Istio with advanced traffic management
- [ ] **Load Testing**: Comprehensive performance testing and optimization
- [ ] **Connection Optimization**: Database and HTTP connection pooling

### Phase 3: Reliability (Weeks 5-6)
- [ ] **Circuit Breakers**: Implement fault tolerance patterns
- [ ] **Graceful Degradation**: Fallback mechanisms for service failures
- [ ] **Disaster Recovery**: Backup, restore, and failover procedures
- [ ] **Security Hardening**: Complete security audit and hardening
- [ ] **Compliance**: HIPAA/healthcare compliance implementation

### Phase 4: Optimization (Weeks 7-8)
- [ ] **Performance Tuning**: Query optimization, caching fine-tuning
- [ ] **Cost Optimization**: Resource right-sizing and scheduling
- [ ] **Advanced Monitoring**: Business metrics, SLI/SLO implementation
- [ ] **Chaos Engineering**: Resilience testing and improvement
- [ ] **Documentation**: Operations runbooks and incident response

## Key Performance Targets

### Throughput Targets
- **Concurrent Users**: 5,000+ simultaneous users
- **Request Rate**: 10,000+ requests/minute peak
- **Assessment Throughput**: 500+ complete assessments/minute
- **LLM Processing**: 1,000+ agent calls/minute

### Latency Targets  
- **API Response**: <200ms p95 (deterministic endpoints)
- **Complete Assessment**: <5s p95 (full agentic flow)
- **Cache Hit Response**: <50ms p95
- **Database Queries**: <100ms p95

### Reliability Targets
- **Uptime**: 99.9% availability (8.76 hours downtime/year)
- **Error Rate**: <0.1% for critical endpoints
- **Recovery Time**: <5 minutes for service failures
- **Data Durability**: 99.999999999% (11 9's)

### Resource Efficiency
- **CPU Utilization**: 60-80% average (headroom for bursts)
- **Memory Utilization**: <85% to prevent OOM kills  
- **Database Connections**: <70% of pool size
- **Cache Hit Rate**: >90% for frequent queries

## UTI Assessment Algorithm
- What is the use case of this algorithm (deterministic) and the integration with agents (non-deterministic)?
- Do we rely fully on the uti assessment algorithm to deterministically generate recommendations and initial assessments, then let the agents check as doctor, pharmacists, and gather evidence for a better assessed treatment filtered for counterarguments and fact-checks?
- What is the point of doctor/pharmacist agents and how do they improve the uti assessment algorithm?
- Are we missing any context or inputs before reaching the uti assessment algorithm?

## Agents
- Current agentic pattern is parallelization and integrated structured output to a single agent via a single model. I think there needs to be some sort of a feedback loop between agents, for instance a UTI doctor agent (urology?) interacts with a hospital pharmacist agent, and pharmacist agent provides feedback to UTI doctor agent for any concerns, and then go to the next step.
- Clinical reasoning, citations, and just extracting claims. Then for each citation, show the rationale of its relevance to its existence. 
- After we produce the final agent, can we produce a final markdown report with all the analysis and decisions we provided? Can we use this to respond to any question the client may have?
- Can we use different models for different agents, like GPT-5 for reasoning parts for agents, web search could use GPT-4.1. 
- [Optional] Agentic memory, yes, for a patient, conditions may evolve over time, we need to remember and update.

## Evaluation and Improvement
- Write a detailed markdown document (heavily descriptive and mermaid diagrams) addressing:
    - How would you evaluate the LLM responses and clinical decisions over time?
    - Who (human reviewers, doctors, patients) would you involve in the evaluation loop and how?
    - How would you use these evaluations to improve the agent's quality and safety?
    - How would you engineer and deploy this eval+improvement system?
- Key thing we are testing: your conceptual understanding of how to evaluate, improve, and deploy ML/LLM/agent systems at scale.